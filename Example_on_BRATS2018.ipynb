{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example on BRATS2018",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavyagitub/faculty-assessment/blob/main/Example_on_BRATS2018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uld1IQgwKIL"
      },
      "source": [
        "Note\n",
        "- [Colab Only] Means the associated cell is required only if you are running the model on Google Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-oHUemNtm59"
      },
      "source": [
        "## Mount Drive [Colab Only]\n",
        "Mount the google drive to access the dataset stored on drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYkOcDYNtf-e",
        "outputId": "b3b7c235-fbd8-4fe8-ff26-45766108fd27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psq2s15_wlfJ"
      },
      "source": [
        "## Extract the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuUurNXQtmXT"
      },
      "source": [
        "import zipfile  # For faster extraction\n",
        "dataset_path = \"/gdrive/My Drive/MICCAI_BraTS_2018_Data_Training.zip\"  # Replace with your dataset path\n",
        "zfile = zipfile.ZipFile(dataset_path)\n",
        "zfile.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NA9UG5zw938"
      },
      "source": [
        "## Get required packages\n",
        "- **SimpleITK**: For loading the dataset\n",
        "- **[model.py](https://github.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/)**: The model from BRATS2018 winning paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqMFpmc6uCjQ",
        "outputId": "1dfae305-6f8c-4ca5-db82-645459b6aab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install simpleitk\n",
        "!wget https://github.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/raw/master/model.py"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpleitk in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "--2025-02-03 11:24:05--  https://github.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/raw/master/model.py\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/master/model.py [following]\n",
            "--2025-02-03 11:24:05--  https://raw.githubusercontent.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/master/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16490 (16K) [text/plain]\n",
            "Saving to: ‘model.py.1’\n",
            "\n",
            "model.py.1          100%[===================>]  16.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-03 11:24:05 (52.1 MB/s) - ‘model.py.1’ saved [16490/16490]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ARH9U4D2Wy"
      },
      "source": [
        "## Imports and helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uAsBShgB3v_",
        "outputId": "94b56044-9efd-4e12-801b-a879010af327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        "import SimpleITK as sitk  # For loading the dataset\n",
        "import numpy as np  # For data manipulation\n",
        "from model import build_model  # For creating the model\n",
        "import glob  # For populating the list of files\n",
        "from scipy.ndimage import zoom  # For resizing\n",
        "import re  # For parsing the filenames (to know their modality)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'mse' from 'keras.losses' (/usr/local/lib/python3.11/dist-packages/keras/api/losses/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-79bc15edd1f8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleITK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msitk\u001b[0m  \u001b[0;31m# For loading the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m  \u001b[0;31m# For data manipulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model\u001b[0m  \u001b[0;31m# For creating the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m  \u001b[0;31m# For populating the list of files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzoom\u001b[0m  \u001b[0;31m# For resizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUpSampling3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpatialDropout3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'mse' from 'keras.losses' (/usr/local/lib/python3.11/dist-packages/keras/api/losses/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAzfzD6MkalL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras implementation of the paper:\n",
        "# 3D MRI Brain Tumor Segmentation Using Autoencoder Regularization\n",
        "# by Myronenko A. (https://arxiv.org/pdf/1810.11654.pdf)\n",
        "# Author of this code: Suyog Jadhav (https://github.com/IAmSUyogJadhav)\n",
        "\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.losses import MeanSquaredError as mse # Import MeanSquaredError and alias it as 'mse'\n",
        "from keras.layers import Conv3D, Activation, Add, UpSampling3D, Lambda, Dense\n",
        "from keras.layers import Input, Reshape, Flatten, Dropout, SpatialDropout3D\n",
        "from keras.optimizers import adam\n",
        "from keras.models import Model\n",
        "try:\n",
        "    from group_norm import GroupNormalization\n",
        "except ImportError:\n",
        "    import urllib.request\n",
        "    print('Downloading group_norm.py in the current directory...')\n",
        "    url = 'https://raw.githubusercontent.com/titu1994/Keras-Group-Normalization/master/group_norm.py'\n",
        "    urllib.request.urlretrieve(url, \"group_norm.py\")\n",
        "    from group_norm import GroupNormalization\n",
        "\n",
        "\n",
        "def green_block(inp, filters, data_format='channels_first', name=None):\n",
        "    \"\"\"\n",
        "    green_block(inp, filters, name=None)\n",
        "    ------------------------------------\n",
        "    Implementation of the special residual block used in the paper. The block\n",
        "    consists of two (GroupNorm --> ReLu --> 3x3x3 non-strided Convolution)\n",
        "    units, with a residual connection from the input `inp` to the output. Used\n",
        "    internally in the model. Can be used independently as well.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `inp`: An keras.layers.layer instance, required\n",
        "        The keras layer just preceding the green block.\n",
        "    `filters`: integer, required\n",
        "        No. of filters to use in the 3D convolutional block. The output\n",
        "        layer of this green block will have this many no. of channels.\n",
        "    `data_format`: string, optional\n",
        "        The format of the input data. Must be either 'chanels_first' or\n",
        "        'channels_last'. Defaults to `channels_first`, as used in the paper.\n",
        "    `name`: string, optional\n",
        "        The name to be given to this green block. Defaults to None, in which\n",
        "        case, keras uses generated names for the involved layers. If a string\n",
        "        is provided, the names of individual layers are generated by attaching\n",
        "        a relevant prefix from [GroupNorm_, Res_, Conv3D_, Relu_, ], followed\n",
        "        by _1 or _2.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    `out`: A keras.layers.Layer instance\n",
        "        The output of the green block. Has no. of channels equal to `filters`.\n",
        "        The size of the rest of the dimensions remains same as in `inp`.\n",
        "    \"\"\"\n",
        "    inp_res = Conv3D(\n",
        "        filters=filters,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format=data_format,\n",
        "        name=f'Res_{name}' if name else None)(inp)\n",
        "\n",
        "    # axis=1 for channels_first data format\n",
        "    # No. of groups = 8, as given in the paper\n",
        "    x = GroupNormalization(\n",
        "        groups=8,\n",
        "        axis=1 if data_format == 'channels_first' else 0,\n",
        "        name=f'GroupNorm_1_{name}' if name else None)(inp)\n",
        "    x = Activation('relu', name=f'Relu_1_{name}' if name else None)(x)\n",
        "    x = Conv3D(\n",
        "        filters=filters,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        data_format=data_format,\n",
        "        name=f'Conv3D_1_{name}' if name else None)(x)\n",
        "\n",
        "    x = GroupNormalization(\n",
        "        groups=8,\n",
        "        axis=1 if data_format == 'channels_first' else 0,\n",
        "        name=f'GroupNorm_2_{name}' if name else None)(x)\n",
        "    x = Activation('relu', name=f'Relu_2_{name}' if name else None)(x)\n",
        "    x = Conv3D(\n",
        "        filters=filters,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        data_format=data_format,\n",
        "        name=f'Conv3D_2_{name}' if name else None)(x)\n",
        "\n",
        "    out = Add(name=f'Out_{name}' if name else None)([x, inp_res])\n",
        "    return out\n",
        "\n",
        "\n",
        "# From keras-team/keras/blob/master/examples/variational_autoencoder.py\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "    z_mean, z_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_var) * epsilon\n",
        "\n",
        "\n",
        "def dice_coefficient(y_true, y_pred):\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=[-3,-2,-1])\n",
        "    dn = K.sum(K.square(y_true) + K.square(y_pred), axis=[-3,-2,-1]) + 1e-8\n",
        "    return K.mean(2 * intersection / dn, axis=[0,1])\n",
        "\n",
        "\n",
        "def loss_gt(e=1e-8):\n",
        "    \"\"\"\n",
        "    loss_gt(e=1e-8)\n",
        "    ------------------------------------------------------\n",
        "    Since keras does not allow custom loss functions to have arguments\n",
        "    other than the true and predicted labels, this function acts as a wrapper\n",
        "    that allows us to implement the custom loss used in the paper. This function\n",
        "    only calculates - L<dice> term of the following equation. (i.e. GT Decoder part loss)\n",
        "\n",
        "    L = - L<dice> + weight_L2 ∗ L<L2> + weight_KL ∗ L<KL>\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `e`: Float, optional\n",
        "        A small epsilon term to add in the denominator to avoid dividing by\n",
        "        zero and possible gradient explosion.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss_gt_(y_true, y_pred): A custom keras loss function\n",
        "        This function takes as input the predicted and ground labels, uses them\n",
        "        to calculate the dice loss.\n",
        "\n",
        "    \"\"\"\n",
        "    def loss_gt_(y_true, y_pred):\n",
        "        intersection = K.sum(K.abs(y_true * y_pred), axis=[-3,-2,-1])\n",
        "        dn = K.sum(K.square(y_true) + K.square(y_pred), axis=[-3,-2,-1]) + e\n",
        "\n",
        "        return - K.mean(2 * intersection / dn, axis=[0,1])\n",
        "\n",
        "    return loss_gt_\n",
        "\n",
        "def loss_VAE(input_shape, z_mean, z_var, weight_L2=0.1, weight_KL=0.1):\n",
        "    \"\"\"\n",
        "    loss_VAE(input_shape, z_mean, z_var, weight_L2=0.1, weight_KL=0.1)\n",
        "    ------------------------------------------------------\n",
        "    Since keras does not allow custom loss functions to have arguments\n",
        "    other than the true and predicted labels, this function acts as a wrapper\n",
        "    that allows us to implement the custom loss used in the paper. This function\n",
        "    calculates the following equation, except for -L<dice> term. (i.e. VAE decoder part loss)\n",
        "\n",
        "    L = - L<dice> + weight_L2 ∗ L<L2> + weight_KL ∗ L<KL>\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "     `input_shape`: A 4-tuple, required\n",
        "        The shape of an image as the tuple (c, H, W, D), where c is\n",
        "        the no. of channels; H, W and D is the height, width and depth of the\n",
        "        input image, respectively.\n",
        "    `z_mean`: An keras.layers.Layer instance, required\n",
        "        The vector representing values of mean for the learned distribution\n",
        "        in the VAE part. Used internally.\n",
        "    `z_var`: An keras.layers.Layer instance, required\n",
        "        The vector representing values of variance for the learned distribution\n",
        "        in the VAE part. Used internally.\n",
        "    `weight_L2`: A real number, optional\n",
        "        The weight to be given to the L2 loss term in the loss function. Adjust to get best\n",
        "        results for your task. Defaults to 0.1.\n",
        "    `weight_KL`: A real number, optional\n",
        "        The weight to be given to the KL loss term in the loss function. Adjust to get best\n",
        "        results for your task. Defaults to 0.1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss_VAE_(y_true, y_pred): A custom keras loss function\n",
        "        This function takes as input the predicted and ground labels, uses them\n",
        "        to calculate the L2 and KL loss.\n",
        "\n",
        "    \"\"\"\n",
        "    def loss_VAE_(y_true, y_pred):\n",
        "        c, H, W, D = input_shape\n",
        "        n = c * H * W * D\n",
        "\n",
        "        loss_L2 = K.mean(K.square(y_true - y_pred), axis=(1, 2, 3, 4)) # original axis value is (1,2,3,4).\n",
        "\n",
        "        loss_KL = (1 / n) * K.sum(\n",
        "            K.exp(z_var) + K.square(z_mean) - 1. - z_var,\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "        return weight_L2 * loss_L2 + weight_KL * loss_KL\n",
        "\n",
        "    return loss_VAE_\n",
        "\n",
        "def build_model(input_shape=(4, 160, 192, 128), output_channels=3, weight_L2=0.1, weight_KL=0.1, dice_e=1e-8):\n",
        "    \"\"\"\n",
        "    build_model(input_shape=(4, 160, 192, 128), output_channels=3, weight_L2=0.1, weight_KL=0.1)\n",
        "    -------------------------------------------\n",
        "    Creates the model used in the BRATS2018 winning solution\n",
        "    by Myronenko A. (https://arxiv.org/pdf/1810.11654.pdf)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `input_shape`: A 4-tuple, optional.\n",
        "        Shape of the input image. Must be a 4D image of shape (c, H, W, D),\n",
        "        where, each of H, W and D are divisible by 2^4, and c is divisible by 4.\n",
        "        Defaults to the crop size used in the paper, i.e., (4, 160, 192, 128).\n",
        "    `output_channels`: An integer, optional.\n",
        "        The no. of channels in the output. Defaults to 3 (BraTS 2018 format).\n",
        "    `weight_L2`: A real number, optional\n",
        "        The weight to be given to the L2 loss term in the loss function. Adjust to get best\n",
        "        results for your task. Defaults to 0.1.\n",
        "    `weight_KL`: A real number, optional\n",
        "        The weight to be given to the KL loss term in the loss function. Adjust to get best\n",
        "        results for your task. Defaults to 0.1.\n",
        "    `dice_e`: Float, optional\n",
        "        A small epsilon term to add in the denominator of dice loss to avoid dividing by\n",
        "        zero and possible gradient explosion. This argument will be passed to loss_gt function.\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    `model`: A keras.models.Model instance\n",
        "        The created model.\n",
        "    \"\"\"\n",
        "    c, H, W, D = input_shape\n",
        "    assert len(input_shape) == 4, \"Input shape must be a 4-tuple\"\n",
        "    assert (c % 4) == 0, \"The no. of channels must be divisible by 4\"\n",
        "    assert (H % 16) == 0 and (W % 16) == 0 and (D % 16) == 0, \\\n",
        "        \"All the input dimensions must be divisible by 16\"\n",
        "\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Encoder\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    ## Input Layer\n",
        "    inp = Input(input_shape)\n",
        "\n",
        "    ## The Initial Block\n",
        "    x = Conv3D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        data_format='channels_first',\n",
        "        name='Input_x1')(inp)\n",
        "\n",
        "    ## Dropout (0.2)\n",
        "    x = SpatialDropout3D(0.2, data_format='channels_first')(x)\n",
        "\n",
        "    ## Green Block x1 (output filters = 32)\n",
        "    x1 = green_block(x, 32, name='x1')\n",
        "    x = Conv3D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        data_format='channels_first',\n",
        "        name='Enc_DownSample_32')(x1)\n",
        "\n",
        "    ## Green Block x2 (output filters = 64)\n",
        "    x = green_block(x, 64, name='Enc_64_1')\n",
        "    x2 = green_block(x, 64, name='x2')\n",
        "    x = Conv3D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        data_format='channels_first',\n",
        "        name='Enc_DownSample_64')(x2)\n",
        "\n",
        "    ## Green Blocks x2 (output filters = 128)\n",
        "    x = green_block(x, 128, name='Enc_128_1')\n",
        "    x3 = green_block(x, 128, name='x3')\n",
        "    x = Conv3D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        data_format='channels_first',\n",
        "        name='Enc_DownSample_128')(x3)\n",
        "\n",
        "    ## Green Blocks x4 (output filters = 256)\n",
        "    x = green_block(x, 256, name='Enc_256_1')\n",
        "    x = green_block(x, 256, name='Enc_256_2')\n",
        "    x = green_block(x, 256, name='Enc_256_3')\n",
        "    x4 = green_block(x, 256, name='x4')\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Decoder\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    ## GT (Groud Truth) Part\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    ### Green Block x1 (output filters=128)\n",
        "    x = Conv3D(\n",
        "        filters=128,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_GT_ReduceDepth_128')(x4)\n",
        "    x = UpSampling3D(\n",
        "        size=2,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_GT_UpSample_128')(x)\n",
        "    x = Add(name='Input_Dec_GT_128')([x, x3])\n",
        "    x = green_block(x, 128, name='Dec_GT_128')\n",
        "\n",
        "    ### Green Block x1 (output filters=64)\n",
        "    x = Conv3D(\n",
        "        filters=64,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_GT_ReduceDepth_64')(x)\n",
        "    x = UpSampling3D(\n",
        "        size=2,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_GT_UpSample_64')(x)\n",
        "    x = Add(name='Input_Dec_GT_64')([x, x2])\n",
        "    x = green_block(x, 64, name='Dec_GT_64')\n",
        "\n",
        "    ### Green Block x1 (output filters=32)\n",
        "    x = Conv3D(\n",
        "        filters=32,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_GT_ReduceDepth_32')(x)\n",
        "    x = UpSampling3D(\n",
        "        size=2,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_GT_UpSample_32')(x)\n",
        "    x = Add(name='Input_Dec_GT_32')([x, x1])\n",
        "    x = green_block(x, 32, name='Dec_GT_32')\n",
        "\n",
        "    ### Blue Block x1 (output filters=32)\n",
        "    x = Conv3D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        data_format='channels_first',\n",
        "        name='Input_Dec_GT_Output')(x)\n",
        "\n",
        "    ### Output Block\n",
        "    out_GT = Conv3D(\n",
        "        filters=output_channels,  # No. of tumor classes is 3\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format='channels_first',\n",
        "        activation='sigmoid',\n",
        "        name='Dec_GT_Output')(x)\n",
        "\n",
        "    ## VAE (Variational Auto Encoder) Part\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    ### VD Block (Reducing dimensionality of the data)\n",
        "    x = GroupNormalization(groups=8, axis=1, name='Dec_VAE_VD_GN')(x4)\n",
        "    x = Activation('relu', name='Dec_VAE_VD_relu')(x)\n",
        "    x = Conv3D(\n",
        "        filters=16,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_VD_Conv3D')(x)\n",
        "\n",
        "    # Not mentioned in the paper, but the author used a Flattening layer here.\n",
        "    x = Flatten(name='Dec_VAE_VD_Flatten')(x)\n",
        "    x = Dense(256, name='Dec_VAE_VD_Dense')(x)\n",
        "\n",
        "    ### VDraw Block (Sampling)\n",
        "    z_mean = Dense(128, name='Dec_VAE_VDraw_Mean')(x)\n",
        "    z_var = Dense(128, name='Dec_VAE_VDraw_Var')(x)\n",
        "    x = Lambda(sampling, name='Dec_VAE_VDraw_Sampling')([z_mean, z_var])\n",
        "\n",
        "    ### VU Block (Upsizing back to a depth of 256)\n",
        "    x = Dense((c//4) * (H//16) * (W//16) * (D//16))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Reshape(((c//4), (H//16), (W//16), (D//16)))(x)\n",
        "    x = Conv3D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_ReduceDepth_256')(x)\n",
        "    x = UpSampling3D(\n",
        "        size=2,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_UpSample_256')(x)\n",
        "\n",
        "    ### Green Block x1 (output filters=128)\n",
        "    x = Conv3D(\n",
        "        filters=128,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_ReduceDepth_128')(x)\n",
        "    x = UpSampling3D(\n",
        "        size=2,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_UpSample_128')(x)\n",
        "    x = green_block(x, 128, name='Dec_VAE_128')\n",
        "\n",
        "    ### Green Block x1 (output filters=64)\n",
        "    x = Conv3D(\n",
        "        filters=64,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_ReduceDepth_64')(x)\n",
        "    x = UpSampling3D(\n",
        "        size=2,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_UpSample_64')(x)\n",
        "    x = green_block(x, 64, name='Dec_VAE_64')\n",
        "\n",
        "    ### Green Block x1 (output filters=32)\n",
        "    x = Conv3D(\n",
        "        filters=32,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_ReduceDepth_32')(x)\n",
        "    x = UpSampling3D(\n",
        "        size=2,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_UpSample_32')(x)\n",
        "    x = green_block(x, 32, name='Dec_VAE_32')\n",
        "\n",
        "    ### Blue Block x1 (output filters=32)\n",
        "    x = Conv3D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        data_format='channels_first',\n",
        "        name='Input_Dec_VAE_Output')(x)\n",
        "\n",
        "    ### Output Block\n",
        "    out_VAE = Conv3D(\n",
        "        filters=4,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        strides=1,\n",
        "        data_format='channels_first',\n",
        "        name='Dec_VAE_Output')(x)\n",
        "\n",
        "    # Build and Compile the model\n",
        "    out = out_GT\n",
        "    model = Model(inp, outputs=[out, out_VAE])  # Create the model\n",
        "    model.compile(\n",
        "        adam(lr=1e-4),\n",
        "        [loss_gt(dice_e), loss_VAE(input_shape, z_mean, z_var, weight_L2=weight_L2, weight_KL=weight_KL)],\n",
        "        metrics=[dice_coefficient]\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "FVGupJUgkn1V",
        "outputId": "b3838bd0-89eb-4487-efb2-766d3c79fc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'adam' from 'keras.optimizers' (/usr/local/lib/python3.11/dist-packages/keras/api/optimizers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a79634a4b23f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUpSampling3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpatialDropout3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'adam' from 'keras.optimizers' (/usr/local/lib/python3.11/dist-packages/keras/api/optimizers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNtloQ9B3sA"
      },
      "source": [
        "def read_img(img_path):\n",
        "    \"\"\"\n",
        "    Reads a .nii.gz image and returns as a numpy array.\n",
        "    \"\"\"\n",
        "    return sitk.GetArrayFromImage(sitk.ReadImage(img_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEIRGXpdfssT"
      },
      "source": [
        "def resize(img, shape, mode='constant', orig_shape=(155, 240, 240)):\n",
        "    \"\"\"\n",
        "    Wrapper for scipy.ndimage.zoom suited for MRI images.\n",
        "    \"\"\"\n",
        "    assert len(shape) == 3, \"Can not have more than 3 dimensions\"\n",
        "    factors = (\n",
        "        shape[0]/orig_shape[0],\n",
        "        shape[1]/orig_shape[1],\n",
        "        shape[2]/orig_shape[2]\n",
        "    )\n",
        "\n",
        "    # Resize to the given shape\n",
        "    return zoom(img, factors, mode=mode)\n",
        "\n",
        "\n",
        "def preprocess(img, out_shape=None):\n",
        "    \"\"\"\n",
        "    Preprocess the image.\n",
        "    Just an example, you can add more preprocessing steps if you wish to.\n",
        "    \"\"\"\n",
        "    if out_shape is not None:\n",
        "        img = resize(img, out_shape, mode='constant')\n",
        "\n",
        "    # Normalize the image\n",
        "    mean = img.mean()\n",
        "    std = img.std()\n",
        "    return (img - mean) / std\n",
        "\n",
        "\n",
        "def preprocess_label(img, out_shape=None, mode='nearest'):\n",
        "    \"\"\"\n",
        "    Separates out the 3 labels from the segmentation provided, namely:\n",
        "    GD-enhancing tumor (ET — label 4), the peritumoral edema (ED — label 2))\n",
        "    and the necrotic and non-enhancing tumor core (NCR/NET — label 1)\n",
        "    \"\"\"\n",
        "    ncr = img == 1  # Necrotic and Non-Enhancing Tumor (NCR/NET)\n",
        "    ed = img == 2  # Peritumoral Edema (ED)\n",
        "    et = img == 4  # GD-enhancing Tumor (ET)\n",
        "\n",
        "    if out_shape is not None:\n",
        "        ncr = resize(ncr, out_shape, mode=mode)\n",
        "        ed = resize(ed, out_shape, mode=mode)\n",
        "        et = resize(et, out_shape, mode=mode)\n",
        "\n",
        "    return np.array([ncr, ed, et], dtype=np.uint8)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piRsc9rYYRzl"
      },
      "source": [
        "## Loading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7adjWzdCuECK"
      },
      "source": [
        "# Get a list of files for all modalities individually\n",
        "t1 = glob.glob('*GG/*/*t1.nii.gz')\n",
        "t2 = glob.glob('*GG/*/*t2.nii.gz')\n",
        "flair = glob.glob('*GG/*/*flair.nii.gz')\n",
        "t1ce = glob.glob('*GG/*/*t1ce.nii.gz')\n",
        "seg = glob.glob('*GG/*/*seg.nii.gz')  # Ground Truth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_WhT40dzlHO"
      },
      "source": [
        "Parse all the filenames and create a dictionary for each patient with structure:\n",
        "\n",
        "{<br />\n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;'t1': _<path to t1 MRI file&gt;_,<br />\n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;'t2': _<path to t2 MRI&gt;_,<br />\n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;'flair': _<path to FLAIR MRI file&gt;_,<br />\n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;'t1ce': _<path to t1ce MRI file&gt;_,<br />\n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;'seg': _<path to Ground Truth file&gt;_,<br />\n",
        "}<br />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dssK9Nmwojp"
      },
      "source": [
        "pat = re.compile('.*_(\\w*)\\.nii\\.gz')\n",
        "\n",
        "data_paths = [{\n",
        "    pat.findall(item)[0]:item\n",
        "    for item in items\n",
        "}\n",
        "for items in list(zip(t1, t2, t1ce, flair, seg))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHqqrbnH0n2E"
      },
      "source": [
        "## Load the data in a Numpy array\n",
        "Creating an empty Numpy array beforehand and then filling up the data helps you gauge beforehand if the data fits in your memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBgbr-R-7Zat"
      },
      "source": [
        "_Loading only the first 4 images here, to save time._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvbbddauz8ij"
      },
      "source": [
        "input_shape = (4, 80, 96, 64)\n",
        "output_channels = 3\n",
        "data = np.empty((len(data_paths[:4]),) + input_shape, dtype=np.float32)\n",
        "labels = np.empty((len(data_paths[:4]), output_channels) + input_shape[1:], dtype=np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4u_yrMf30k4",
        "outputId": "81c1cb4d-8339-41bb-971c-a386bf3c7f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import math\n",
        "\n",
        "# Parameters for the progress bar\n",
        "total = len(data_paths[:4])\n",
        "step = 25 / total\n",
        "\n",
        "for i, imgs in enumerate(data_paths[:4]):\n",
        "    try:\n",
        "        data[i] = np.array([preprocess(read_img(imgs[m]), input_shape[1:]) for m in ['t1', 't2', 't1ce', 'flair']], dtype=np.float32)\n",
        "        labels[i] = preprocess_label(read_img(imgs['seg']), input_shape[1:])[None, ...]\n",
        "\n",
        "        # Print the progress bar\n",
        "        print('\\r' + f'Progress: '\n",
        "            f\"[{'=' * int((i+1) * step) + ' ' * (24 - int((i+1) * step))}]\"\n",
        "            f\"({math.ceil((i+1) * 100 / (total))} %)\",\n",
        "            end='')\n",
        "    except Exception as e:\n",
        "        print(f'Something went wrong with {imgs[\"t1\"]}, skipping...\\n Exception:\\n{str(e)}')\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: [=========================](100 %)"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojc7YgwC305t"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR_SDmgn4Lrd"
      },
      "source": [
        "build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI53S6yZJN2V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "c6b380e3-c011-44a2-dc38-a1977883d518"
      },
      "source": [
        "model = build_model(input_shape=input_shape, output_channels=3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'build_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fd3cae6f2c1a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'build_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1u107O74NOP"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0y7NfU4JBgi",
        "outputId": "9f506516-2c7b-460e-aa5a-7abdab4eceb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "model.fit(data, [labels, data], batch_size=1, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "4/4 [==============================] - 29s 7s/step - loss: 0.3766 - acc: 0.1188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f227c43bc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlrqEF7m7ist"
      },
      "source": [
        "That's it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKerVpLQTRck"
      },
      "source": [
        "## Closing Regards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkSxJbRx4SsO"
      },
      "source": [
        "If you are resizing the segmentation mask, the resized segmentation mask retains the overall shape, but loses a lot of pixels and becomes somewhat 'grainy'. See the illustration below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAau6Z7hznFC"
      },
      "source": [
        "1. Original segmentation mask:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXRW65_hkL9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "4c42e648-b9fe-4176-a324-b15ca9be7db5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "img = (read_img(seg[0])[100] == 2).astype(np.uint8)\n",
        "print(img.shape)\n",
        "print(np.unique(img))\n",
        "print(img.sum())\n",
        "plt.imshow(img, cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(240, 240)\n",
            "[0 1]\n",
            "2111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2275c25be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soDTIJFn5CE9"
      },
      "source": [
        "After resizing to (80, 96, 64)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kmgpXjsAMzj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "7eac8776-abc2-4334-a5b2-ffba151be588"
      },
      "source": [
        "img = preprocess_label(read_img(seg[0]), out_shape=(80, 96, 64), mode='nearest')[1][50]\n",
        "print(img.shape)\n",
        "print(np.unique(img))\n",
        "print(img.sum())\n",
        "plt.imshow(img, cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(96, 64)\n",
            "[0 1]\n",
            "116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2275d5bc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALYAAAD8CAYAAADaM14OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiGSgdqA5LGd"
      },
      "source": [
        "One can clearly notice that there are now a lot of black pixels in the region where there should have been only white pixels. This can potentially hurt our model. So, it is best to not resize the image too much. But, due to computational constraints and the model requirements, it is unavoidable.\n",
        "\n",
        "However, given below are a few things one could try to reduce the downsampling noise as much as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01FtsxQi6dts"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbflvVYqz2yu"
      },
      "source": [
        "- Original Image > preprocess_label > Morphological Closing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFn6tiXPsUdl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "b7474d73-2f00-4c25-8968-4f740e5a2510"
      },
      "source": [
        "kernel = np.ones((3, 3))\n",
        "img_closed = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=3)\n",
        "print(np.unique(img_closed))\n",
        "print(img_closed.sum())\n",
        "plt.imshow(img_closed, cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n",
            "491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f227dc26160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17CKK5zQz7hs"
      },
      "source": [
        "- Original Image > preprocess_label > Morphological Dilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxhKa3kKtCLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "7ee710b6-c5ed-4e9c-e2f0-935357e1bf0c"
      },
      "source": [
        "kernel = np.ones((3, 3))\n",
        "img_dilated = cv2.dilate(img, kernel, iterations=1)\n",
        "print(np.unique(img_dilated))\n",
        "print(img_dilated.sum())\n",
        "plt.imshow(img_dilated, cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n",
            "660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f227db77d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG3vfTyS0BVe"
      },
      "source": [
        "You could try these things to get even better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX6Cj1VO6vrZ"
      },
      "source": [
        "## Feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPt_15Ps6vO1"
      },
      "source": [
        "If you have any feedback, queries, bug reports to send, please feel free to [raise an issue](https://github.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/issues/new) on github. It would be really helpful!"
      ]
    }
  ]
}